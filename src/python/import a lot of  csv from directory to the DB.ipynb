{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How to download a lot of csv to the POSTGRES\n",
    "\n",
    "This is not the ideal script/code , but maybe you will find some usefull tips \n",
    "\n",
    "Method is using scripts to get the list of the csv, and then by pandas import it to the DB (by my openion in pretty fast way )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "List of csv in the desired directory\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "from pathlib import Path \n",
    "path = '/Users/alexandrbesan/pathwithCSV'\n",
    "onlyfiles = [f for f in listdir(path) if isfile(join(path, f)) and Path(f).suffix=='.csv']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filelistRules = list()\n",
    "import os\n",
    "def is_non_zero_file(fpath):  \n",
    "    return os.path.isfile(fpath) and os.path.getsize(fpath) > 0\n",
    "\n",
    "for i in onlyfiles:\n",
    "    file = path+'/'+i\n",
    "    try: \n",
    "        with open(file, encoding='utf-8-sig') as f:\n",
    "            first_line = f.readline()  \n",
    "            dtype = {each_string.lower():str for each_string in first_line.replace('\\n','').split(';')}\n",
    "            columns = [each_string.lower() for each_string in first_line.replace('\\n','').split(';')]\n",
    "            isFileProcessed = True \n",
    "    except: \n",
    "        dtype = {}\n",
    "        columns = {}\n",
    "        isFileProcessed = False \n",
    "    fileDescr = {'dtype':dtype, 'filename':i,'tablename': i.replace('.csv','').lower() , 'fullPath':file ,'columns':columns ,'status':False, 'isFileProcessed': isFileProcessed}\n",
    "    if(is_non_zero_file(path+'/'+i)):\n",
    "        filelistRules.append(fileDescr)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the files that get an issue (wrong format, maybe there is no column names , etc.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(filter(lambda p : p['isFileProcessed'] ==False, filelistRules))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "connection to the DB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sqlalchemy import create_engine\n",
    "db_conn = create_engine('postgresql://login:password@server:5432/db')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the structure and CREATE statement of destination tables by using pd.io.sql.get_schema for the list above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def SQL_CREATE_STATEMENT_FROM_DATAFRAME(SOURCE, TARGET):\n",
    "    import pandas as pd\n",
    "    sql_text = pd.io.sql.get_schema(SOURCE.reset_index(drop=True), TARGET)   \n",
    "    return sql_text\n",
    "\n",
    "\n",
    "import pandas as pd \n",
    "for i in filelistRules:  \n",
    "    df = pd.read_csv(   i['fullPath']\n",
    "                    ,   header=0\n",
    "                    ,   names=i['columns']\n",
    "                    ,   delimiter=';'  \n",
    "                    ,   dtype = i['dtype']\n",
    "                    ,   encoding='cp1251' \n",
    "                    ,   nrows=20\n",
    "                    )  \n",
    "    target = 'schema.' + i['tablename']\n",
    "    sql = SQL_CREATE_STATEMENT_FROM_DATAFRAME(df, target).replace('\"','').replace('CREATE TABLE','CREATE TABLE IF NOT EXISTS ')\n",
    "    print(sql)\n",
    "    db_conn.execute(sql)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "There is to method of importing \n",
    "- just copy_from \n",
    "- import by the  chunksize to the DB by pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import psycopg2\n",
    "import codecs \n",
    "import io\n",
    "db_connection = psycopg2.connect('postgresql://login:password@server:5432/db')\n",
    "db_cursor = db_connection.cursor()\n",
    "for i in filelistRules:  \n",
    "    target = 'schema.' + i['tablename']\n",
    "    try: \n",
    "        if(not i['status']): \n",
    "            print('method copy_from, table=', target)\n",
    "            f = codecs.open(i['fullPath'], 'r', encoding='cp1251')\n",
    "            f.readline() \n",
    "            db_cursor.copy_from(f, target, sep=\";\") \n",
    "            f.close()     \n",
    "            db_cursor.connection.commit()\n",
    "            i['status']=True\n",
    "    except: \n",
    "        if(not i['status']): \n",
    "            print('method pandas, table=', target)\n",
    "            buffer = io.StringIO()\n",
    "            df = pd.read_csv(   i['fullPath']\n",
    "                        ,   header=0\n",
    "                        ,   names=i['columns']\n",
    "                        ,   delimiter=';'\n",
    "                        ,   encoding='cp1251' )  \n",
    "                        \n",
    "            df.to_csv(buffer, header=False, index = False)\n",
    "            buffer.seek(0)\n",
    "            db_cursor.copy_from(buffer, target, sep=\",\") \n",
    "            db_cursor.connection.commit()\n",
    "            \"\"\"\n",
    "            df.to_sql(\n",
    "                i['tablename'], \n",
    "                db_conn, \n",
    "                schema='schema',\n",
    "                index=False ,  \n",
    "                if_exists='append',\n",
    "                chunksize=1000000,\n",
    "                method = 'multi'\n",
    "            )     \n",
    "            \"\"\"\n",
    "            i['status']=True\n",
    "db_cursor.close()\n",
    "db_connection.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code below is the import for specific cases. Thats csv - with errors ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import psycopg2\n",
    "import codecs \n",
    "import io\n",
    "db_connection = psycopg2.connect('postgresql://login:password@server:5432/db')\n",
    "db_cursor = db_connection.cursor()\n",
    "for i in filelistRules:  \n",
    "    if(not i['status']): \n",
    "        target = 'schema.' + i['tablename'] \n",
    "        print( target)\n",
    "        buffer = io.StringIO()\n",
    "        df = pd.read_csv(   i['fullPath']\n",
    "                    ,   header=0\n",
    "                    ,   names=i['columns']\n",
    "                    ,   delimiter=';'\n",
    "                    ,   dtype = i['dtype']\n",
    "                    ,   encoding='cp1251' )  \n",
    "        df.replace(to_replace=[r\"\\\\t|\\\\n|\\\\r\", \"\\t|\\n|\\r\"], value=[\"\",\"\"], regex=True,inplace=True)  \n",
    "        df.replace(to_replace=r'\\\\', value=\"\", regex=True,inplace=True)  \n",
    "        df.to_csv(buffer, header=False, index = False, sep =\";\")\n",
    "        buffer.seek(0)\n",
    "        db_cursor.copy_from(buffer, target, sep=\";\") \n",
    "        db_cursor.connection.commit() \n",
    "        i['status']=True\n",
    "db_cursor.close()\n",
    "db_connection.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "delete all the tables from the list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clear all tables\n",
    "# Attention! Script drop tables from the list \n",
    "import psycopg2\n",
    "import codecs \n",
    "import io\n",
    "db_connection = psycopg2.connect('postgresql://login:password@server:5432/db')\n",
    "db_cursor = db_connection.cursor()\n",
    "for i in filelistRules:  \n",
    "    target = 'schema.' + i['tablename']\n",
    "    db_cursor.execute('drop table  IF EXISTS '+ target +';') \n",
    "    print('drop table  IF EXISTS '+ target +';')\n",
    "    db_cursor.connection.commit() \n",
    "db_cursor.close()\n",
    "db_connection.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# references \n",
    "1. https://medium.com/analytics-vidhya/part-4-pandas-dataframe-to-postgresql-using-python-8ffdb0323c09\n",
    "2. https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.to_csv.html\n",
    "3. https://newbedev.com/copy-data-from-csv-to-postgresql-using-python\n",
    "4. https://www.psycopg.org/docs/cursor.html#cursor.copy_expert\n",
    "5. https://www.psycopg.org/docs/usage.html#copy\n",
    "6. https://enigma.com/blog/post/scaling-a-pandas-etl-job-to-600gb\n",
    "7. https://pandas.pydata.org/pandas-docs/dev/reference/api/pandas.read_csv.html\n",
    "8. https://towardsdatascience.com/upload-your-pandas-dataframe-to-your-database-10x-faster-eb6dc6609ddf"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "44fa308324eea5e0aa7654f09614f03763bad9fed6c1164f742085b734bc861d"
  },
  "kernelspec": {
   "display_name": "Python 3.7.11 64-bit ('miniconda3-4.7.12': pyenv)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
